{
  "name": "sparknode",
  "version": "0.3.2",
  "description": "Interface with a spark core through the spark.io cloud api.",
  "main": "index.js",
  "repository": {
    "type": "git",
    "url": "https://github.com/andrewstuart/Sparknode"
  },
  "keywords": [
    "sparkcore",
    "spark",
    "core",
    "microcontroller",
    "wifi",
    "embedded"
  ],
  "author": {
    "name": "Andrew Stuart"
  },
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/andrewstuart/Sparknode/issues"
  },
  "homepage": "https://github.com/andrewstuart/Sparknode",
  "dependencies": {
    "lodash": "~2.4.1",
    "commander": "~2.1.0"
  },
  "devDependencies": {
    "mocha": "~1.17.0",
    "nock": "~0.27.1"
  },
  "bin": {
    "spark": "./bin/cli.js"
  },
  "readme": "#About\n\nThis package was built with the purpose of allowing cross-platform communication from node.js to a [sparkcore](http://www.spark.io) through the [spark Cloud API](http://docs.spark.io/#/api).\nFirmware functions and variables are added automatically based on the spark API's HATEOS responses.\n\nThere are two main components: The module and the [CLI](#cli).\n\n###[Example](#example-1)\n\n#Constructors\n1. [Core](#core)\n2. [Collection](#collection)\n\n##Core\nCreate a new instance of a spark core. First parameter is authtoken, second parameter is deviceId.\n\nAn object can also be used as the first parameter, as follows:\n\n```javascript\nvar spark = require('sparknode');\n\nvar core = new Core({\n  authtoken: <Your Auth_Token>,\n  id: <Your device id>\n});\n```\n\n###Cloud Functions\nEach function accepts a string as the parameter, and a callback to be called upon return.\n\nThe signature of the callback should be `function(err, data)`.\n\n```javascript\ncore.brew('coffee', function(err, data) {\n  console.log(data);\n});\n```\n\n###Cloud Variables\nEach variable (exposed as functions) accepts a callback as its first parameter, with the same signature as above (err, data).\n\n```javascript\ncore.variable1(function(err, data) {\n  console.log(data);\n});\n```\n\nVariables also have a property, autoupdate, that can be set with a timeout in milliseconds between each call, or true to use the default 5 second interval or false to cancel autoupdates. Setting this variable will also start the update cycle.\n\nWhen being used with autoupdate, the variable (not the core) will fire an `'update'` event each time a response is received from the server.\n\n```javascript\n//Start autoupdate\ncore.variable1.autoupdate = 2000;\n\n//Do something on update\ncore.variable1.on('update', function(value) {\n  console.log(value);\n  console.log(core.variable1.value);\n});\n\n//Stop update with any falsy value.\ncore.variable1.autoupdate = false;\n\n```\n\nThe last known value can be retreived as a property (value) of the function.\n\n```javascript\nconsole.log(core.variable1.value);\n```\n\n##Collection\nEven better, get all your spark cores at once, complete with everything they can do.\n\nOnce loaded, the collection instance contains all your spark cores by name.\n\n```javascript\ncollection.core1.doFunction('foo', function(err, data) {\n  //Do something\n});\n```\n\nThe default behavior is to cache the output of the cloud api for all HATEOS calls in a JSON object at your project root.  If you'd like to override this behavior, you can pass an options object (optional, of course) to the constructor.\n\n```javascript\nvar collection = new Collection(myAuthToken, { skipCache: true })\n```\nor\n```javascript\nvar collection = new Collection(myAuthToken, { cacheFile: 'lib/cacheFile.json' } )\n```\n\n##Example\n\n```javascript\nvar sp = require('sparknode');\nvar collection = new sp.Collection(myAuthToken);\ncollection.on('connect', function() {\n  //Turn on an led\n  collection.core1.digitalwrite('d7,HIGH');\n\n  //Brew some coffee, then email me.\n  collection.core2.brew('coffee', function(err, timeUntilFinished) {\n    if(err) {\n      throw err;\n    }\n\n    setTimeout(function() {\n      //General awesomeness goes here.\n      emailMe();\n      sendSocketIoMessage();\n      addCreamer();\n    }, timeUntilFinished);\n  })\n\n  //Get a variable\n  collection.core2.remainingCoffeeTime(function(err, value) {\n    //Do something with value\n  })\n```\n\nAnd an example of a single core.\n\n```javascript\nvar randomCore = new sp.Core(myAuthToken, deviceId);\n\nrandomCore.on('connect', function() {\n  randomCore.turnOnLights();\n});\n```\n\n#CLI\n\nIf installed globally via `npm install -g sparknode`, sparknode will give you a command line interface mostly useful for debugging, but I suppose it could be used for other scripting applications as well.\n\nThe most important command is probably `spark -h`, as it lets you discover the functionality directly from the command line.\n\nAs for the rest, right now there are three main commands under the main `spark` command: `add`, `fn`, and `var`. Each of these also have help generated with the -h switch.\n\n####add\nSpark add will retreive any cores accessible via the given token. These are saved at your home directory under .sparkrc.\n\nSyntax is `spark add <token>`.\n\n####var\nRetreive a variable from the spark cloud. \n\nSyntax is `spark var coreName [varName]`. If no `varName` is included, the list of registered variables will be printed.\n\nOptions include:\n\n-n Number of times to check the variable (--number)\n\n-i Interval, in milliseconds, between checks (--interval)\n\n-c Check continously at interval or 1 second. (will override -n) (--continuous)\n\n####fn\nExecute a remote function and print the return value. If no `functionName` is included, the list of registered functions will be printed.\n\nSyntax is `spark fn <coreName> <functionName> <argument>`.\n\n####ls\nGet either a list of cores and their functions or a single core.\n\nSyntax: `spark ls [coreName]`\n\nOutput Example:\n\n```bash\nspark ls\nCore: core1 (1234567890abcdef12345678)\n\nFunctions: \nbrew\ndigitalread\ndigitalwrite\nanalogread\n\nVariables: \ndelay\n\nConnected: true\n\n-----------\n\nCore: core2 (1234567890abcdef12345679)\n\nFunctions: \ngetdata\ndigitalread\ndigitalwrite\nanalogread\n\nVariables: \ndelay\n\nConnected: true\n\n-----------\n\n```\n\n##CLI Examples\n\n  ```bash\n#Go get all the cores.\n  spark add 1234567890abcdef1234567890abcdef\n\n#The following cores were found and saved: \n#core1 1234425432363457d\n#core2 1212532454325acef\n\n  spark fn core1\n\n#Functions available for core 'core1':\n#  brew\n#  digitalread\n#  digitalwrite\n#  analogread\n\n  spark fn core1 brew coffee;\n#1\n\n  spark fn core2\n#  digitalwrite\n\n  spark fn core2 digitalwrite \"A1,HIGH\";\n\n  spark var core1\n\n#Variables available for core 'core1':\n#  brewTime\n#  variable1\n\n  spark var core1 brewTime;\n\n#  120000\n\n  spark var core2\n\n#Variables available for core 'core2':\n#  coffeeStrength\n\n  spark var -i 100 -n 5 core2 coffeeStrength;\n\n#100\n#100\n#98\n#99\n#96\n\n#My current personal favorite:\n  spark var -ci 100 core1 variable1;\n\n#1\n#2\n#3\n#4\n#5\n#...\n  ```\n\n\n##Future\n\n  Future:\n\n  An API for the server sent events will also be a high priority as soon as that cloud API comes out.\n\n  I'd like to write a custom firmware that's CLI flashable and uses TCP directly for faster feedback. You're already using Node, so you have that option. It should be possible to write very powerful client-server code using something like this. I'd also like to keep it modular so it's useful on its own.\n\n  I'm also thinking about writing a custom firmware that lets you add many more than 4 functions, directly from the CLI or even programmatically, using string parsing on the client side. I don't know about anyone else, but I don't need 64 characters of input very often, so I figured they'd be more useful this way. Check out the issues tracker to add feature requests and see some of the plans I have.\n",
  "readmeFilename": "README.md",
  "_id": "sparknode@0.3.2",
  "dist": {
    "shasum": "a3d03db981a4a69c420a54a9d55246d4a57e8b39"
  },
  "_from": "sparknode@",
  "_resolved": "https://registry.npmjs.org/sparknode/-/sparknode-0.3.2.tgz"
}
